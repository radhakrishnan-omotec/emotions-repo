{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 127160,
          "sourceType": "datasetVersion",
          "datasetId": 64677
        }
      ],
      "dockerImageVersionId": 30408,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhakrishnan-omotec/emotions-repo/blob/main/Aarav_Shah_Project_1_Emotion_Recognition_Facial_Expressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'facial-expression-recognitionferchallenge:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F64677%2F127160%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240705%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240705T094012Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D95480086efeb6ccb6023eb209511f39190db06a7cc80a16975635040c35232c623abd5f78c63f81fbd82cb061511ad2ee9b3e9e19d38ea062a0676d231e909b00da0fd9c23638fef58b25d66b920e0e7c71415467a230f9f5e2e59194d39bc9a138587ad4fe206974b3bdc8a764f3772a718e7f967d65b2ebed758225cbfa5036df322f3282a54bfe5413f4e91408dcefffd79a52cde8b22bc0a1a4e028a13948ccc988fde66eebbeb294af8e979a63e43958595bcbe4cedaf77f534ee6f36e05ad04c6c0442cc6643a98f020ef4d8b8ce4e3d5d8cb90b267677c60e2e23417cb673c2984b55a1e51eab9eb7952f059f6db91c080e3d85ccbdcbef6366716415'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "406XjdeIRBFq",
        "outputId": "d548ec4d-c717-4a8f-f100-ecfa204dd026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading facial-expression-recognitionferchallenge, 101283289 bytes compressed\n",
            "[==================================================] 101283289 bytes downloaded\n",
            "Downloaded and uncompressed: facial-expression-recognitionferchallenge\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Emotion Recognition using Facial Expressions**"
      ],
      "metadata": {
        "id": "KQZnoBVZRBFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Importing Required Libraries"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.00803,
          "end_time": "2023-02-21T23:32:28.775887",
          "exception": false,
          "start_time": "2023-02-21T23:32:28.767857",
          "status": "completed"
        },
        "tags": [],
        "id": "TTMAac35RBFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cv2\n",
        "import scikitplot\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "papermill": {
          "duration": 8.188704,
          "end_time": "2023-02-21T23:32:36.972552",
          "exception": false,
          "start_time": "2023-02-21T23:32:28.783848",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:20:30.159903Z",
          "iopub.execute_input": "2023-02-26T10:20:30.160647Z",
          "iopub.status.idle": "2023-02-26T10:20:40.355172Z",
          "shell.execute_reply.started": "2023-02-26T10:20:30.160608Z",
          "shell.execute_reply": "2023-02-26T10:20:40.353841Z"
        },
        "trusted": true,
        "id": "lWEqEYSERBFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data Preprocessing"
      ],
      "metadata": {
        "id": "xVUc6mArRBFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('../input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "papermill": {
          "duration": 5.936801,
          "end_time": "2023-02-21T23:32:42.918178",
          "exception": false,
          "start_time": "2023-02-21T23:32:36.981377",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:20:40.357208Z",
          "iopub.execute_input": "2023-02-26T10:20:40.357914Z",
          "iopub.status.idle": "2023-02-26T10:20:47.201413Z",
          "shell.execute_reply.started": "2023-02-26T10:20:40.35788Z",
          "shell.execute_reply": "2023-02-26T10:20:47.20029Z"
        },
        "trusted": true,
        "id": "k3B1m-B7RBFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.emotion.unique()"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.026804,
          "end_time": "2023-02-21T23:32:42.954541",
          "exception": false,
          "start_time": "2023-02-21T23:32:42.927737",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:20:47.203122Z",
          "iopub.execute_input": "2023-02-26T10:20:47.203511Z",
          "iopub.status.idle": "2023-02-26T10:20:47.220663Z",
          "shell.execute_reply.started": "2023-02-26T10:20:47.203481Z",
          "shell.execute_reply": "2023-02-26T10:20:47.21936Z"
        },
        "trusted": true,
        "id": "2H-zzMDsRBFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text to numerical labeling\n",
        "emotions_labels = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01742,
          "end_time": "2023-02-21T23:32:42.980389",
          "exception": false,
          "start_time": "2023-02-21T23:32:42.962969",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:20:47.224024Z",
          "iopub.execute_input": "2023-02-26T10:20:47.224438Z",
          "iopub.status.idle": "2023-02-26T10:20:47.229614Z",
          "shell.execute_reply.started": "2023-02-26T10:20:47.224399Z",
          "shell.execute_reply": "2023-02-26T10:20:47.228383Z"
        },
        "trusted": true,
        "id": "pc2qqtUHRBF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.emotion.value_counts()"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.022323,
          "end_time": "2023-02-21T23:32:43.011341",
          "exception": false,
          "start_time": "2023-02-21T23:32:42.989018",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:20:47.231888Z",
          "iopub.execute_input": "2023-02-26T10:20:47.232372Z",
          "iopub.status.idle": "2023-02-26T10:20:47.245078Z",
          "shell.execute_reply.started": "2023-02-26T10:20:47.232334Z",
          "shell.execute_reply": "2023-02-26T10:20:47.243792Z"
        },
        "trusted": true,
        "id": "OIHSYxJeRBF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-26T10:20:47.247512Z",
          "iopub.execute_input": "2023-02-26T10:20:47.24801Z",
          "iopub.status.idle": "2023-02-26T10:20:47.273999Z",
          "shell.execute_reply.started": "2023-02-26T10:20:47.247974Z",
          "shell.execute_reply": "2023-02-26T10:20:47.272296Z"
        },
        "trusted": true,
        "id": "Y93RhXUfRBF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the side length of the images\n",
        "math.sqrt(len(df.pixels[0].split(' ')))"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.01875,
          "end_time": "2023-02-21T23:32:43.038771",
          "exception": false,
          "start_time": "2023-02-21T23:32:43.020021",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:20:47.275533Z",
          "iopub.execute_input": "2023-02-26T10:20:47.276052Z",
          "iopub.status.idle": "2023-02-26T10:20:47.286238Z",
          "shell.execute_reply.started": "2023-02-26T10:20:47.276008Z",
          "shell.execute_reply": "2023-02-26T10:20:47.284854Z"
        },
        "trusted": true,
        "id": "ZRmxT-T-RBF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Viewing the images\n",
        "fig = pyplot.figure(1, (14, 14))\n",
        "\n",
        "k = 0\n",
        "for label in sorted(df.emotion.unique()):\n",
        "    for j in range(7):\n",
        "        px = df[df.emotion==label].pixels.iloc[k]\n",
        "        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')\n",
        "\n",
        "        k += 1\n",
        "        ax = pyplot.subplot(7, 7, k)\n",
        "        ax.imshow(px, cmap = 'gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_title(emotions_labels[label])\n",
        "        pyplot.tight_layout()"
      ],
      "metadata": {
        "papermill": {
          "duration": 7.540714,
          "end_time": "2023-02-21T23:32:50.588115",
          "exception": false,
          "start_time": "2023-02-21T23:32:43.047401",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:21:41.631384Z",
          "iopub.execute_input": "2023-02-26T10:21:41.632457Z",
          "iopub.status.idle": "2023-02-26T10:21:50.04694Z",
          "shell.execute_reply.started": "2023-02-26T10:21:41.632401Z",
          "shell.execute_reply": "2023-02-26T10:21:50.045917Z"
        },
        "trusted": true,
        "id": "vdCISbVGRBF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanations of each lines:\n",
        "\n",
        "* fig = pyplot.figure(1, (14, 14)): This line creates a new figure with figure number 1 and dimensions of 14 inches by 14 inches.\n",
        "* k = 0: This initializes a counter variable k to zero.\n",
        "* for label in sorted(df.emotion.unique()):: This loops over the unique emotion labels in the emotion column of the df DataFrame, sorted in ascending order.\n",
        "* for j in range(7):: This nested loop creates 7 columns of subplots for each emotion category.\n",
        "* px = df[df.emotion==label].pixels.iloc[k]: This line selects the k-th image from the pixels column of the df DataFrame that belongs to the current label emotion category, and stores it in a variable px.\n",
        "* px = np.array(px.split(' ')).reshape(48, 48).astype('float32'): This line converts the string of pixel values into a NumPy array, reshapes it into a 48x48 array, and converts it to a float32 data type.\n",
        "* k += 1: This increments the k counter variable by 1.\n",
        "* ax = pyplot.subplot(7, 7, k): This creates a new subplot with a 7x7 grid layout, and selects the k-th subplot as the current one to plot the current image.\n",
        "* ax.imshow(px, cmap = 'gray'): This displays the image data stored in px on the current subplot, using a grayscale color map.\n",
        "* ax.set_xticks([]): This removes the x-axis tick labels from the current subplot.\n",
        "* ax.set_yticks([]): This removes the y-axis tick labels from the current subplot.\n",
        "* ax.set_title(emotion_label_to_text[label]): This sets the title of the current subplot to the corresponding emotion label, as specified in a dictionary emotion_label_to_text.\n",
        "* pyplot.tight_layout(): This adjusts the layout of the subplots to prevent overlapping."
      ],
      "metadata": {
        "id": "ABcgZaX8RBF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next step is to make the data ready for neural networks"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.012414,
          "end_time": "2023-02-21T23:32:50.613392",
          "exception": false,
          "start_time": "2023-02-21T23:32:50.600978",
          "status": "completed"
        },
        "tags": [],
        "id": "2Na22s0sRBF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that the data df contains pixel values stored as strings."
      ],
      "metadata": {
        "id": "ZsxZlDWKRBF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48).astype('float32'))\n",
        "img_array = np.stack(img_array, axis = 0)"
      ],
      "metadata": {
        "papermill": {
          "duration": 39.660137,
          "end_time": "2023-02-21T23:33:30.286412",
          "exception": false,
          "start_time": "2023-02-21T23:32:50.626275",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:21:55.823665Z",
          "iopub.execute_input": "2023-02-26T10:21:55.824698Z",
          "iopub.status.idle": "2023-02-26T10:22:39.338641Z",
          "shell.execute_reply.started": "2023-02-26T10:21:55.824642Z",
          "shell.execute_reply": "2023-02-26T10:22:39.337358Z"
        },
        "trusted": true,
        "id": "HNGiuVnMRBF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48).astype('float32')): This line uses the Pandas apply() function to apply a lambda function to each element in the pixels column of the df DataFrame. The lambda function splits the string of pixel values into an array of strings using the space character as a delimiter, converts the resulting array into a 48x48 NumPy array of float32 data type, and returns the array. The resulting output is a Pandas Series of NumPy arrays.\n",
        "* img_array = np.stack(img_array, axis = 0): This line uses the NumPy stack() function to stack the arrays in the Pandas Series along the 0-th axis, creating a 3D NumPy array of shape (n_samples, 48, 48), where n_samples is the number of samples in the original DataFrame. This creates a NumPy array of image data that can be used as input for machine learning models."
      ],
      "metadata": {
        "id": "4ogdFHbyRBF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Viewing the shape of image array created\n",
        "img_array.shape"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.025222,
          "end_time": "2023-02-21T23:33:30.324733",
          "exception": false,
          "start_time": "2023-02-21T23:33:30.299511",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:22:51.480259Z",
          "iopub.execute_input": "2023-02-26T10:22:51.481036Z",
          "iopub.status.idle": "2023-02-26T10:22:51.488068Z",
          "shell.execute_reply.started": "2023-02-26T10:22:51.480995Z",
          "shell.execute_reply": "2023-02-26T10:22:51.486856Z"
        },
        "trusted": true,
        "id": "15FH5cAbRBF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_features = []\n",
        "\n",
        "for i in range(len(img_array)):\n",
        "    temp = cv2.cvtColor(img_array[i], cv2.COLOR_GRAY2RGB)\n",
        "    img_features.append(temp)\n",
        "\n",
        "img_features = np.array(img_features)\n",
        "print(img_features.shape)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.912211,
          "end_time": "2023-02-21T23:33:31.25095",
          "exception": false,
          "start_time": "2023-02-21T23:33:30.338739",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:22:53.840093Z",
          "iopub.execute_input": "2023-02-26T10:22:53.841117Z",
          "iopub.status.idle": "2023-02-26T10:22:54.830446Z",
          "shell.execute_reply.started": "2023-02-26T10:22:53.841073Z",
          "shell.execute_reply": "2023-02-26T10:22:54.828992Z"
        },
        "trusted": true,
        "id": "fPdRTUffRBF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each grayscale image in img_array is converted to an RGB image using OpenCV's cvtColor() function with the COLOR_GRAY2RGB flag. This results in a list of RGB images stored in img_features.\n",
        "\n",
        "Note: It is not always necessary to convert a 3D NumPy array of grayscale images into a 4D NumPy array of RGB images for face emotion recognition using TensorFlow. However, some convolutional neural network (CNN) architectures may require input images to have three color channels (i.e., RGB format) instead of one (i.e., grayscale format). In such cases, converting grayscale images to RGB format is necessary.\n",
        "In particular, some CNN architectures are designed to work with RGB images and use convolutional filters that expect input tensors with three color channels. If grayscale images are used as input instead, the filters may not be able to learn relevant features from the input images, leading to poor performance."
      ],
      "metadata": {
        "id": "zuvS-HzpRBF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Showing an image from those\n",
        "pyplot.imshow(img_features[400].astype(np.uint8));"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.22468,
          "end_time": "2023-02-21T23:33:31.489635",
          "exception": false,
          "start_time": "2023-02-21T23:33:31.264955",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:19.324829Z",
          "iopub.execute_input": "2023-02-26T10:31:19.325294Z",
          "iopub.status.idle": "2023-02-26T10:31:19.556427Z",
          "shell.execute_reply.started": "2023-02-26T10:31:19.325242Z",
          "shell.execute_reply": "2023-02-26T10:31:19.555237Z"
        },
        "trusted": true,
        "id": "-KqHZVSfRBF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thw shown image is Black & White and you may confused why it is bw even when we converted to 4D with RGB channels. This is because, the image is actually black and white: If the original grayscale image was predominantly black and white, then the resulting RGB image created by duplicating the grayscale values across all three color channels would also be mostly black and white."
      ],
      "metadata": {
        "id": "TPEGeTGqRBF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "\n",
        "img_labels = le.fit_transform(df.emotion)\n",
        "img_labels = np_utils.to_categorical(img_labels)\n",
        "img_labels.shape"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.028227,
          "end_time": "2023-02-21T23:33:31.531431",
          "exception": false,
          "start_time": "2023-02-21T23:33:31.503204",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:22.526938Z",
          "iopub.execute_input": "2023-02-26T10:31:22.527392Z",
          "iopub.status.idle": "2023-02-26T10:31:22.541762Z",
          "shell.execute_reply.started": "2023-02-26T10:31:22.527353Z",
          "shell.execute_reply": "2023-02-26T10:31:22.540624Z"
        },
        "trusted": true,
        "id": "vouAVqbjRBF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code block, the LabelEncoder class from Scikit-learn is used to convert the categorical labels in the df.emotion column to numerical labels, which are then converted to one-hot encoded binary vectors using the np_utils.to_categorical() function from Keras.\n",
        "The resulting img_labels array is a 2D NumPy array of shape (n_samples, n_classes), where n_samples is the number of images in the dataset and n_classes is the number of unique labels in the dataset. Each row in the array corresponds to a single image and contains a binary vector representing the label of the image.\n",
        "The img_labels array is used as the target variable in a supervised learning model, where the goal is to predict the label of a new image given its pixel values."
      ],
      "metadata": {
        "id": "mD2SRq1rRBF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(le_name_mapping)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.023406,
          "end_time": "2023-02-21T23:33:31.568248",
          "exception": false,
          "start_time": "2023-02-21T23:33:31.544842",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:24.873161Z",
          "iopub.execute_input": "2023-02-26T10:31:24.873834Z",
          "iopub.status.idle": "2023-02-26T10:31:24.884708Z",
          "shell.execute_reply.started": "2023-02-26T10:31:24.873767Z",
          "shell.execute_reply": "2023-02-26T10:31:24.883312Z"
        },
        "trusted": true,
        "id": "xieQGNEqRBF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block creates a dictionary le_name_mapping that maps each unique label in the df.emotion column to a numerical label using the LabelEncoder class from Scikit-learn.\n",
        "\n",
        "The zip() function is used to pair each unique label with its corresponding numerical label, which is obtained using the le.transform() method of the LabelEncoder object le. The resulting pairs are then passed to the dict() function to create a dictionary that maps each unique label to its numerical label.\n",
        "\n",
        "The resulting le_name_mapping dictionary can be useful for interpreting the output of a machine learning model that uses the numerical labels as its predicted outputs. By mapping the predicted numerical label back to its corresponding categorical label using le_name_mapping, we can interpret the output of the model in terms of the original categories.\n",
        "\n",
        "For example, if the model predicts a numerical label of 3, we can use le.inverse_transform([3]) to obtain the corresponding categorical label, such as 'happy'. If we also have le_name_mapping, we can simply look up the label using le_name_mapping[3] to obtain 'happy'."
      ],
      "metadata": {
        "id": "JOlEcudkRBGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Training the model"
      ],
      "metadata": {
        "id": "eGYgql1ZRBGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and Test split of data\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(img_features, img_labels, shuffle = True, stratify = img_labels, test_size = 0.1, random_state = 42)\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.552102,
          "end_time": "2023-02-21T23:33:32.159866",
          "exception": false,
          "start_time": "2023-02-21T23:33:31.607764",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:29.926979Z",
          "iopub.execute_input": "2023-02-26T10:31:29.927414Z",
          "iopub.status.idle": "2023-02-26T10:31:30.507537Z",
          "shell.execute_reply.started": "2023-02-26T10:31:29.927376Z",
          "shell.execute_reply": "2023-02-26T10:31:30.505786Z"
        },
        "trusted": true,
        "id": "a7WdQJXFRBGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_width = X_train.shape[1]\n",
        "img_height = X_train.shape[2]\n",
        "img_depth = X_train.shape[3]\n",
        "num_classes = y_train.shape[1]"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.023031,
          "end_time": "2023-02-21T23:33:32.236334",
          "exception": false,
          "start_time": "2023-02-21T23:33:32.213303",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:31.186357Z",
          "iopub.execute_input": "2023-02-26T10:31:31.187134Z",
          "iopub.status.idle": "2023-02-26T10:31:31.1952Z",
          "shell.execute_reply.started": "2023-02-26T10:31:31.187096Z",
          "shell.execute_reply": "2023-02-26T10:31:31.193639Z"
        },
        "trusted": true,
        "id": "RBiQcC-XRBGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next step is to normalize the results since neural networks are very sensitive to unnormalized data."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.013873,
          "end_time": "2023-02-21T23:33:32.263975",
          "exception": false,
          "start_time": "2023-02-21T23:33:32.250102",
          "status": "completed"
        },
        "tags": [],
        "id": "rr9JnToTRBGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255.\n",
        "X_valid = X_valid / 255."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.30647,
          "end_time": "2023-02-21T23:33:32.584567",
          "exception": false,
          "start_time": "2023-02-21T23:33:32.278097",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:33.703209Z",
          "iopub.execute_input": "2023-02-26T10:31:33.703658Z",
          "iopub.status.idle": "2023-02-26T10:31:34.000658Z",
          "shell.execute_reply.started": "2023-02-26T10:31:33.70362Z",
          "shell.execute_reply": "2023-02-26T10:31:33.999364Z"
        },
        "trusted": true,
        "id": "cMyNuPLpRBGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading VGG19 convolutional neural network (CNN) from Keras\n",
        "vgg19 = tf.keras.applications.VGG19(weights = 'imagenet', include_top = False, input_shape = (48, 48, 3))"
      ],
      "metadata": {
        "papermill": {
          "duration": 4.097025,
          "end_time": "2023-02-21T23:33:36.695651",
          "exception": false,
          "start_time": "2023-02-21T23:33:32.598626",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:35.094908Z",
          "iopub.execute_input": "2023-02-26T10:31:35.096212Z",
          "iopub.status.idle": "2023-02-26T10:31:39.664013Z",
          "shell.execute_reply.started": "2023-02-26T10:31:35.09616Z",
          "shell.execute_reply": "2023-02-26T10:31:39.662805Z"
        },
        "trusted": true,
        "id": "T_AWPumsRBGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making all layers of VGG19 model non-trainable\n",
        "for layer in vgg19.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.021894,
          "end_time": "2023-02-21T23:33:36.760159",
          "exception": false,
          "start_time": "2023-02-21T23:33:36.738265",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:39.666202Z",
          "iopub.execute_input": "2023-02-26T10:31:39.666665Z",
          "iopub.status.idle": "2023-02-26T10:31:39.673371Z",
          "shell.execute_reply.started": "2023-02-26T10:31:39.666621Z",
          "shell.execute_reply": "2023-02-26T10:31:39.672149Z"
        },
        "trusted": true,
        "id": "ayyH50hWRBGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a model is marked as non-trainable, the weights of its layers are fixed and remain unchanged during training. In other words, the model will not adjust its weights based on the current training data.\n",
        "\n",
        "Setting the layers of the pre-trained VGG19 model to be non-trainable has the effect of using the pre-learned features of the model for the new problem at hand, without updating the model's weights based on the new data. This is called transfer learning, and it allows us to use a pre-trained model for a new task, which may have fewer training examples or a different distribution of data than the original task.\n",
        "\n",
        "Using transfer learning with a pre-trained model like VGG19 is generally considered a good practice in deep learning, especially when the amount of data available for the new problem is limited.\n",
        "\n",
        "By using pre-trained weights, we can take advantage of the model's ability to learn meaningful features from images, without having to train a new model from scratch. This saves time and computational resources, and can often lead to better results than training a new model with limited data.\n",
        "\n",
        "However, the performance of the pre-trained model on the new task depends on the similarity of the new data to the data on which the model was originally trained. If the new data is very different, the pre-trained weights may not be useful, and it may be necessary to fine-tune the model's weights on the new data. In some cases, it may also be necessary to modify the architecture of the pre-trained model to better suit the new task.\n",
        "\n",
        "In the context of facial emotion recognition using the VGG19 model, we are taking advantage of the pre-learned features of the VGG19 model for image classification, and adapting it for our specific task of recognizing facial emotions. By keeping the pre-trained weights fixed, we can use the power of transfer learning to improve the performance of our model on our specific task."
      ],
      "metadata": {
        "id": "-jZo2ZetRBGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg19.summary()"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.10045,
          "end_time": "2023-02-21T23:33:36.874709",
          "exception": false,
          "start_time": "2023-02-21T23:33:36.774259",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:40.018327Z",
          "iopub.execute_input": "2023-02-26T10:31:40.019615Z",
          "iopub.status.idle": "2023-02-26T10:31:40.073107Z",
          "shell.execute_reply.started": "2023-02-26T10:31:40.019563Z",
          "shell.execute_reply": "2023-02-26T10:31:40.07217Z"
        },
        "trusted": true,
        "id": "T57XdxkwRBGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(bottom_model, classes):\n",
        "    model = bottom_model.layers[-2].output\n",
        "    model = GlobalAveragePooling2D()(model)\n",
        "    model = Dense(classes, activation = 'softmax', name = 'out_layer')(model)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.025758,
          "end_time": "2023-02-21T23:33:36.917291",
          "exception": false,
          "start_time": "2023-02-21T23:33:36.891533",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:43.587352Z",
          "iopub.execute_input": "2023-02-26T10:31:43.588098Z",
          "iopub.status.idle": "2023-02-26T10:31:43.594151Z",
          "shell.execute_reply.started": "2023-02-26T10:31:43.588058Z",
          "shell.execute_reply": "2023-02-26T10:31:43.59286Z"
        },
        "trusted": true,
        "id": "eO2RdPqQRBGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function first takes the output of the second-to-last layer of the pre-trained model, which contains high-level features learned from the input images, and passes it through a global average pooling layer. This layer takes the average of each feature map in the input and reduces the spatial dimensions of the output to 1x1, while retaining the channel dimension.\n",
        "\n",
        "The output of the global average pooling layer is then passed through a fully connected Dense layer with a softmax activation function, which outputs a probability distribution over the possible classes of facial emotions.\n",
        "\n",
        "The resulting model has all layers of the pre-trained model frozen, and only the newly added layers are trainable. This is because the pre-trained weights are already optimized for a specific task (image classification on ImageNet), and we want to preserve these weights while only adjusting the new layers for the task of facial emotion recognition."
      ],
      "metadata": {
        "id": "M7iqcIdZRBGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head = build_model(vgg19, num_classes)\n",
        "model = Model(inputs = vgg19.input, outputs = head)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.122994,
          "end_time": "2023-02-21T23:33:37.057333",
          "exception": false,
          "start_time": "2023-02-21T23:33:36.934339",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:46.19912Z",
          "iopub.execute_input": "2023-02-26T10:31:46.199881Z",
          "iopub.status.idle": "2023-02-26T10:31:46.237942Z",
          "shell.execute_reply.started": "2023-02-26T10:31:46.199841Z",
          "shell.execute_reply": "2023-02-26T10:31:46.235292Z"
        },
        "trusted": true,
        "id": "ohnta1EzRBGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#'early stopping' for avoiding overfitting training data and `ReduceLROnPlateau` for learning rate\n",
        "\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy',\n",
        "                               min_delta = 0.00005,\n",
        "                               patience = 11,\n",
        "                               verbose = 1,\n",
        "                               restore_best_weights = True,)\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                 factor = 0.5,\n",
        "                                 patience = 7,\n",
        "                                 min_lr = 1e-7,\n",
        "                                 verbose = 1,)\n",
        "\n",
        "callbacks = [early_stopping,lr_scheduler,]"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.028579,
          "end_time": "2023-02-21T23:33:37.148178",
          "exception": false,
          "start_time": "2023-02-21T23:33:37.119599",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:47.322479Z",
          "iopub.execute_input": "2023-02-26T10:31:47.323147Z",
          "iopub.status.idle": "2023-02-26T10:31:47.331547Z",
          "shell.execute_reply.started": "2023-02-26T10:31:47.323108Z",
          "shell.execute_reply": "2023-02-26T10:31:47.329898Z"
        },
        "trusted": true,
        "id": "n5BnDFxJRBGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating more training data\n",
        "train_datagen = ImageDataGenerator(rotation_range = 15,\n",
        "                                   width_shift_range = 0.15,\n",
        "                                   height_shift_range = 0.15,\n",
        "                                   shear_range = 0.15,\n",
        "                                   zoom_range = 0.15,\n",
        "                                   horizontal_flip = True,)\n",
        "train_datagen.fit(X_train)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.355921,
          "end_time": "2023-02-21T23:33:37.573936",
          "exception": false,
          "start_time": "2023-02-21T23:33:37.218015",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:48.423127Z",
          "iopub.execute_input": "2023-02-26T10:31:48.424287Z",
          "iopub.status.idle": "2023-02-26T10:31:48.75951Z",
          "shell.execute_reply.started": "2023-02-26T10:31:48.424214Z",
          "shell.execute_reply": "2023-02-26T10:31:48.758285Z"
        },
        "trusted": true,
        "id": "IaVz8YspRBGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 25\n",
        "optims = [optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.9, beta_2 = 0.999),]\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = optims[0],\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.038507,
          "end_time": "2023-02-21T23:33:37.632452",
          "exception": false,
          "start_time": "2023-02-21T23:33:37.593945",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:31:49.685359Z",
          "iopub.execute_input": "2023-02-26T10:31:49.687593Z",
          "iopub.status.idle": "2023-02-26T10:31:49.711421Z",
          "shell.execute_reply.started": "2023-02-26T10:31:49.687536Z",
          "shell.execute_reply": "2023-02-26T10:31:49.710265Z"
        },
        "trusted": true,
        "id": "DPUn3ziDRBGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets the hyperparameters for training the facial emotion recognition model.\n",
        "\n",
        "batch_size specifies the number of samples to use in each iteration of training. In this case, 32 samples will be used in each iteration.\n",
        "\n",
        "epochs specifies the number of times to iterate over the entire training dataset. In this case, the model will be trained for 25 epochs.\n",
        "\n",
        "optims is a list of optimizers to use during training. In this case, it contains a single optimizer: Adam. Adam is a popular optimizer that adapts the learning rate during training based on the gradient history.\n",
        "\n",
        "Finally, the code compiles the model with the specified loss function (categorical_crossentropy), optimizer (optims[0]), and metric to evaluate during training (accuracy)."
      ],
      "metadata": {
        "id": "-QYtipdERBGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_datagen.flow(X_train,\n",
        "                                       y_train,\n",
        "                                       batch_size = batch_size),\n",
        "                                       validation_data = (X_valid, y_valid),\n",
        "                                       steps_per_epoch = len(X_train) / batch_size,\n",
        "                                       epochs = epochs,\n",
        "                                       callbacks = callbacks,\n",
        "                                       use_multiprocessing = True)"
      ],
      "metadata": {
        "papermill": {
          "duration": 1530.626395,
          "end_time": "2023-02-21T23:59:08.278254",
          "exception": false,
          "start_time": "2023-02-21T23:33:37.651859",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T10:34:23.049385Z",
          "iopub.execute_input": "2023-02-26T10:34:23.050035Z",
          "iopub.status.idle": "2023-02-26T10:59:04.946029Z",
          "shell.execute_reply.started": "2023-02-26T10:34:23.049996Z",
          "shell.execute_reply": "2023-02-26T10:59:04.941818Z"
        },
        "trusted": true,
        "id": "L-rAydtWRBGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_yaml = model.to_json()\n",
        "with open(\"model.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "\n",
        "model.save(\"model.h5\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 1.983505,
          "end_time": "2023-02-21T23:59:11.647964",
          "exception": false,
          "start_time": "2023-02-21T23:59:09.664459",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T11:18:42.751033Z",
          "iopub.execute_input": "2023-02-26T11:18:42.752014Z",
          "iopub.status.idle": "2023-02-26T11:18:42.928778Z",
          "shell.execute_reply.started": "2023-02-26T11:18:42.751968Z",
          "shell.execute_reply": "2023-02-26T11:18:42.927625Z"
        },
        "trusted": true,
        "id": "3_0sUynfRBGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set()\n",
        "fig = pyplot.figure(0, (12, 4))\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 1)\n",
        "sns.lineplot(x=history.epoch, y=history.history['accuracy'], label='train')\n",
        "sns.lineplot(x=history.epoch, y=history.history['val_accuracy'], label='valid')\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 2)\n",
        "sns.lineplot(x=history.epoch, y=history.history['loss'], label='train')\n",
        "sns.lineplot(x=history.epoch, y=history.history['val_loss'], label='valid')\n",
        "pyplot.title('Loss')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "pyplot.savefig('epoch_history_dcnn.png')\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 2.74917,
          "end_time": "2023-02-21T23:59:15.824774",
          "exception": false,
          "start_time": "2023-02-21T23:59:13.075604",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T11:18:45.134893Z",
          "iopub.execute_input": "2023-02-26T11:18:45.1353Z",
          "iopub.status.idle": "2023-02-26T11:18:45.99762Z",
          "shell.execute_reply.started": "2023-02-26T11:18:45.135262Z",
          "shell.execute_reply": "2023-02-26T11:18:45.996495Z"
        },
        "trusted": true,
        "id": "d4FdXhsCRBGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_accu = pd.DataFrame({'train': history.history['accuracy'], 'valid': history.history['val_accuracy']})\n",
        "df_loss = pd.DataFrame({'train': history.history['loss'], 'valid': history.history['val_loss']})\n",
        "\n",
        "fig = pyplot.figure(0, (14, 4))\n",
        "ax = pyplot.subplot(1, 2, 1)\n",
        "sns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), showfliers=False)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "ax = pyplot.subplot(1, 2, 2)\n",
        "sns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), showfliers=False)\n",
        "pyplot.title('Loss')\n",
        "pyplot.tight_layout()\n",
        "\n",
        "pyplot.savefig('performance_dist.png')\n",
        "pyplot.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": 2.126464,
          "end_time": "2023-02-21T23:59:19.924749",
          "exception": false,
          "start_time": "2023-02-21T23:59:17.798285",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T11:18:50.348478Z",
          "iopub.execute_input": "2023-02-26T11:18:50.349501Z",
          "iopub.status.idle": "2023-02-26T11:18:51.012318Z",
          "shell.execute_reply.started": "2023-02-26T11:18:50.349407Z",
          "shell.execute_reply": "2023-02-26T11:18:51.011187Z"
        },
        "trusted": true,
        "id": "tj2QZqfHRBGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat_valid = np.argmax(model.predict(X_valid), axis=1)\n",
        "scikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid, axis=1), yhat_valid, figsize=(7,7))\n",
        "pyplot.savefig(\"confusion_matrix_dcnn.png\")\n",
        "\n",
        "print(f'total wrong validation predictions: {np.sum(np.argmax(y_valid, axis=1) != yhat_valid)}\\n\\n')\n",
        "print(classification_report(np.argmax(y_valid, axis=1), yhat_valid))"
      ],
      "metadata": {
        "papermill": {
          "duration": 4.659401,
          "end_time": "2023-02-21T23:59:25.995629",
          "exception": false,
          "start_time": "2023-02-21T23:59:21.336228",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T11:18:54.055046Z",
          "iopub.execute_input": "2023-02-26T11:18:54.056259Z",
          "iopub.status.idle": "2023-02-26T11:18:57.768013Z",
          "shell.execute_reply.started": "2023-02-26T11:18:54.05619Z",
          "shell.execute_reply": "2023-02-26T11:18:57.766636Z"
        },
        "trusted": true,
        "id": "CNSpUj1KRBGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The confusion matrix indicates that our model is performing well on the \"happy\" class, but its performance is lacking on the other two classes. A potential explanation for this could be the limited amount of data available for these classes. However, upon reviewing the images, I noticed that certain images from these classes were even challenging for a human to differentiate between \"sad\" and \"neutral.\" Additionally, facial expressions can vary among individuals, and some people's neutral faces may appear sad."
      ],
      "metadata": {
        "papermill": {
          "duration": 1.555314,
          "end_time": "2023-02-21T23:59:29.246947",
          "exception": false,
          "start_time": "2023-02-21T23:59:27.691633",
          "status": "completed"
        },
        "tags": [],
        "id": "0oQ4HNNRRBGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapper = {\n",
        "    0: 'anger',\n",
        "    1: 'disgust',\n",
        "    2: 'fear',\n",
        "    3: 'happiness',\n",
        "    4: 'sadness',\n",
        "    5: 'surprise',\n",
        "    6: 'neutral'\n",
        "}"
      ],
      "metadata": {
        "papermill": {
          "duration": 1.48513,
          "end_time": "2023-02-21T23:59:32.142503",
          "exception": false,
          "start_time": "2023-02-21T23:59:30.657373",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T11:19:58.642056Z",
          "iopub.execute_input": "2023-02-26T11:19:58.643044Z",
          "iopub.status.idle": "2023-02-26T11:19:58.648966Z",
          "shell.execute_reply.started": "2023-02-26T11:19:58.642989Z",
          "shell.execute_reply": "2023-02-26T11:19:58.647699Z"
        },
        "trusted": true,
        "id": "r5qQuh0JRBGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2)\n",
        "random_sad_imgs = np.random.choice(np.where(y_valid[:, 1]==1)[0], size=9)\n",
        "random_neutral_imgs = np.random.choice(np.where(y_valid[:, 2]==1)[0], size=9)\n",
        "\n",
        "fig = pyplot.figure(1, (18, 4))\n",
        "\n",
        "for i, (sadidx, neuidx) in enumerate(zip(random_sad_imgs, random_neutral_imgs)):\n",
        "        ax = pyplot.subplot(2, 9, i+1)\n",
        "        sample_img = X_valid[sadidx,:,:,0]\n",
        "        ax.imshow(sample_img, cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        sample_img = cv2.cvtColor(sample_img, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        ax.set_title(f\"true:sad, pred:{mapper[np.argmax(model.predict(sample_img.reshape(1,48,48,3))[0])]}\")\n",
        "\n",
        "        ax = pyplot.subplot(2, 9, i+10)\n",
        "        sample_img = X_valid[neuidx,:,:,0]\n",
        "        ax.imshow(sample_img, cmap='gray')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        sample_img = cv2.cvtColor(sample_img, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        ax.set_title(f\"t:neut, p:{mapper[np.argmax(model.predict(sample_img.reshape(1,48,48,3))[0])]}\")\n",
        "\n",
        "        pyplot.tight_layout()"
      ],
      "metadata": {
        "papermill": {
          "duration": 4.399358,
          "end_time": "2023-02-21T23:59:38.017405",
          "exception": false,
          "start_time": "2023-02-21T23:59:33.618047",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-02-26T11:20:05.949931Z",
          "iopub.execute_input": "2023-02-26T11:20:05.950376Z",
          "iopub.status.idle": "2023-02-26T11:20:08.964048Z",
          "shell.execute_reply.started": "2023-02-26T11:20:05.950337Z",
          "shell.execute_reply": "2023-02-26T11:20:08.962922Z"
        },
        "trusted": true,
        "id": "jzgYi9yuRBGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}